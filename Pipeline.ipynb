{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123aa769-824f-4301-80cc-ee7b19afe01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.35.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.7 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/43.7 kB 165.2 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 41.0/43.7 kB 393.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 238.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.5 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/9.5 MB 2.0 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/9.5 MB 2.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/9.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/9.5 MB 2.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.8/9.5 MB 3.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.5 MB 3.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.2/9.5 MB 3.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.7/9.5 MB 4.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.9/9.5 MB 4.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.3/9.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.7/9.5 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.2/9.5 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.3/9.5 MB 5.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.7/9.5 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.1/9.5 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.5/9.5 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.9/9.5 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.6/9.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.8/9.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.9/9.5 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.2/9.5 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.6/9.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.9/9.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.2/9.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.2/9.5 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.5 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.7/9.5 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.5 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.5 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.5 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.5 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.5 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.5 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.0/9.5 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.0/9.5 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.0/9.5 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.3/9.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.4/9.5 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "   ---------------------------------------- 0.0/417.5 kB ? eta -:--:--\n",
      "   --------------------------------------  409.6/417.5 kB 12.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  409.6/417.5 kB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 417.5/417.5 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.4/2.2 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed huggingface-hub-0.24.6 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.99)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 112.6/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 112.6/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 235.5/991.5 kB 1.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 368.6/991.5 kB 1.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 512.0/991.5 kB 1.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 512.0/991.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 696.3/991.5 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 696.3/991.5 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 706.6/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 739.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 737.3/991.5 kB 451.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 778.2/991.5 kB 254.6 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 205.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 809.0/991.5 kB 178.7 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------ 819.2/991.5 kB 156.4 kB/s eta 0:00:02\n",
      "   -------------------------------- ----- 839.7/991.5 kB 146.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ----- 839.7/991.5 kB 146.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ----- 839.7/991.5 kB 146.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ----- 839.7/991.5 kB 146.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ----- 839.7/991.5 kB 146.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ----- 839.7/991.5 kB 146.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ----- 839.7/991.5 kB 146.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ----- 860.2/991.5 kB 141.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 860.2/991.5 kB 141.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 860.2/991.5 kB 141.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 860.2/991.5 kB 141.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 860.2/991.5 kB 141.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 860.2/991.5 kB 141.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 860.2/991.5 kB 141.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 860.2/991.5 kB 141.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 134.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 134.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 134.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 134.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 134.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 134.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 134.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 134.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/991.5 kB 130.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/991.5 kB 130.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/991.5 kB 130.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/991.5 kB 130.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 890.9/991.5 kB 130.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  983.0/991.5 kB 138.7 kB/s eta 0:00:01\n",
      "   -------------------------------------  983.0/991.5 kB 138.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 991.5/991.5 kB 138.3 kB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.99\n",
      "    Uninstalling sentencepiece-0.1.99:\n",
      "      Successfully uninstalled sentencepiece-0.1.99\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (2023.10.3)\n",
      "Requirement already satisfied: click in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/897.5 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/897.5 kB 325.1 kB/s eta 0:00:03\n",
      "   ---- --------------------------------- 102.4/897.5 kB 737.3 kB/s eta 0:00:02\n",
      "   ---- --------------------------------- 102.4/897.5 kB 737.3 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 225.3/897.5 kB 981.9 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 225.3/897.5 kB 981.9 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 471.0/897.5 kB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 665.6/897.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  880.6/897.5 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  890.9/897.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  890.9/897.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 897.5/897.5 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\te20312262\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U sentencepiece\n",
    "!pip install -U sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55eab740-b619-4b0f-ba7c-981e4865c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\te20312262\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c481d-ea8b-46b6-b078-e5fc8f8af22f",
   "metadata": {},
   "source": [
    "## Text classification pipleline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a69fb44-24a0-4bc9-853e-77c6e9083b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\te20312262\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\",model=\"michellejieli/emotion_text_classifier\")\n",
    "text =\"wow! we have came this far\"\n",
    "outputs = classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efdcd7b-dd69-48d3-8725-3b1da458cd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>surprise</td>\n",
       "      <td>0.951917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  surprise  0.951917"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf89f31-d1b7-4be7-90e6-e96b944bc659",
   "metadata": {},
   "source": [
    "## NER pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5d23b87-f746-4543-84d3-d89db189b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('ner',aggregation_strategy='simple')\n",
    "text=\"I am teja\"\n",
    "outputs = classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b0b7d00-cd2f-4b8e-b1d4-041a66330bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.4415876, 'word': 'te', 'start': 5, 'end': 7}]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3ca16-80b4-4285-a8ea-d198c5604591",
   "metadata": {},
   "source": [
    "## Question Answering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce500fe2-88e5-4b54-a2e3-cb773be757f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text ='''\n",
    "Dear Amazon, last week I ordered an Optimus Price action figure from your\n",
    "online store in India. Unfortunately when I opened the package, I discovered to\n",
    "my horror that I had been sent an action figure of Megatron instead!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbe83b54-50f2-4aaf-b263-19ed01c98282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"what was wrong?\"\n",
    "\n",
    "outputs = reader(question=question, context=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dea6224a-ef56-4cf8-9eb0-f50b16ac0ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.2956933379173279,\n",
       " 'start': 170,\n",
       " 'end': 222,\n",
       " 'answer': 'I had been sent an action figure of Megatron instead'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd02983-dc80-4f41-8c95-6a298f9045e3",
   "metadata": {},
   "source": [
    "## Summarization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5a9397f-e075-4f08-a6ae-7b481c7a1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b8f571936f4f33a05b6ca026f5d908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a224efc372647f89961fd8e48cfca7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2cf65447b14dea8a6da0ad46a8f157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4e09af7e9b41c08c94e52ab11cdc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b0ec8608974a1aa855b01402cde889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\te20312262\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Your max_length is set to 142, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Amazon sent an Optimus Price action figure from your online store in India . Unfortunately when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! Amazon.com has been sending a figure of Optimus Price instead of Optimus Megatron .'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce5382-ff6a-4865-99c3-09873633d2d8",
   "metadata": {},
   "source": [
    "## Text generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "035495b5-3f23-4913-ae73-5d3745f6935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f75c3d0-6b28-4982-b855-4d3145292f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "response = \"I am sorry to hear that your order was mixed up\"\n",
    "\n",
    "prompt = \"user: \"+ text + \" Customer Service response: \" + response\n",
    "\n",
    "outputs = generator(prompt, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d64ce16-e5e6-4e7a-bfcf-0d16f67cfdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"user: \\nDear Amazon, last week I ordered an Optimus Price action figure from your\\nonline store in India. Unfortunately when I opened the package, I discovered to\\nmy horror that I had been sent an action figure of Megatron instead!\\n Customer Service response: I am sorry to hear that your order was mixed up with many online orders.\\nSo here is what I was going to do about it. First, we've got two extra sets of Megatron, and second, I am ordering a brand new model Optimus and a G2 (because you are one of the people who received this one).\\nAs you all\"}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0659cfd-aa63-4c48-a017-27fdca1cc345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
